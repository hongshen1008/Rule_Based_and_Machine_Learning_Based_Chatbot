{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26c69a93-2969-411c-b599-03c9c8ae7716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hfyhc5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hfyhc5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\hfyhc5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hfyhc5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://medium.com/@rr_42830/build-your-own-chatbot-using-deep-learning-23a022638067\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d4c17e-2c83-47ee-ab53-9f17dba8b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(data):\n",
    "    # extract data from json file\n",
    "    training_sentences = []\n",
    "    training_labels = []\n",
    "    labels = []\n",
    "    responses = []\n",
    "\n",
    "\n",
    "    for intent in data['intents']:\n",
    "        for pattern in intent['patterns']:\n",
    "            training_sentences.append(pattern)\n",
    "            training_labels.append(intent['tag'])\n",
    "        responses.append(intent['responses'])\n",
    "\n",
    "        if intent['tag'] not in labels:\n",
    "            labels.append(intent['tag'])\n",
    "\n",
    "    num_classes = len(labels)\n",
    "\n",
    "    # fit transform to BOW\n",
    "    lbl_encoder = LabelEncoder() # convert categorical variables to numerical labels (\"greeting\", \"bye\", \"thanks\") -> (0, 1, 2)\n",
    "    lbl_encoder.fit(training_labels) \n",
    "    training_labels = lbl_encoder.transform(training_labels)\n",
    "    training_labels = keras.utils.to_categorical(training_labels, num_classes=num_classes) # convert to bow model (binary representation)\n",
    "\n",
    "    vocab_size = 1000\n",
    "    embedding_dim = 16\n",
    "    max_len = 20\n",
    "    oov_token = \"<OOV>\" # out of vocabulary token value\n",
    "\n",
    "    # tokenizing text, converting words to lowercase, filtering out punctuation, and converting text into sequences of integers\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token) \n",
    "    tokenizer.fit_on_texts(training_sentences)\n",
    "    word_index = tokenizer.word_index\n",
    "    sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "    padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)\n",
    "    \n",
    "    train_data = padded_sequences\n",
    "    train_labels = training_labels\n",
    "    \n",
    "    # LSTM model\n",
    "    model = keras.Sequential([\n",
    "        Embedding(len(word_index)+1, embedding_dim, input_length=max_len),\n",
    "        LSTM(embedding_dim),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(train_data, train_labels, epochs=150, verbose=0)\n",
    "    \n",
    "    return model, lbl_encoder, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a59daaf-86a6-48ab-8414-4709b59762d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train model\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy', \n",
    "#               optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # model.summary()\n",
    "\n",
    "\n",
    "# epochs = 500\n",
    "# history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)\n",
    "\n",
    "# # to save the trained model\n",
    "# model.save(\"chat_model\")\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# # to save the fitted tokenizer\n",
    "# with open('tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# # to save the fitted label encoder\n",
    "# with open('label_encoder.pickle', 'wb') as ecn_file:\n",
    "#     pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a23f1bc-f200-40b1-8145-f1709b4d2de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lstmModel(padded_sequences, training_labels, word_index, embedding_dim):\n",
    "#     train_data = padded_sequences\n",
    "#     train_labels = training_labels\n",
    "\n",
    "#     model = keras.Sequential([\n",
    "#         Embedding(len(word_index)+1, embedding_dim, input_length=max_len),\n",
    "#         LSTM(embedding_dim),\n",
    "#         Dense(num_classes, activation=\"softmax\")\n",
    "#     ])\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=\"rmsprop\",\n",
    "#         loss=\"categorical_crossentropy\",\n",
    "#         metrics=[\"accuracy\"]\n",
    "#     )\n",
    "\n",
    "#     model.fit(train_data, train_labels, epochs=150, verbose=0)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f84d03-34ac-479b-a8f7-450f38d48a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openMenu():\n",
    "    with open('Beverage_Menu.txt', 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "621e59da-b668-46f3-ab8a-81f5251b660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name Entity Recognision (NER)\n",
    "def NER(query):\n",
    "    name_entity = []\n",
    "    # temp_greeting = list(greet_in)\n",
    "    # if(query != ''):\n",
    "    #     for word in nltk.word_tokenize(query):\n",
    "    #         for item in temp_greeting: #loop through greet_in\n",
    "    #                 item = item.capitalize() #capitaize each item in greet_in\n",
    "    #                 if (word==item):\n",
    "    #                     query = query.replace(word,'') #remove greeeting\n",
    "    words =  nltk.word_tokenize(query)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    chunked = nltk.ne_chunk(tagged, binary=False)\n",
    "    for chunk in chunked.leaves():\n",
    "        if hasattr(chunk, 'label') or chunk[1] == 'NNP':\n",
    "            # name_entity = named_entity + ' ' + ' '.join(chunk[0])\n",
    "            name_entity.append(chunk[0])\n",
    "                \n",
    "    userName = ' ' + ' '.join(name for name in name_entity)\n",
    "    \n",
    "    # nameDict = {\"name\":userName}\n",
    "    # storeKnowledge(nameDict)\n",
    "    return userName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a052994-5970-48a4-b5a1-16680e738c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectMisspelledWord(query, menuPrice):\n",
    "    # !pip install fuzzywuzzy\n",
    "    from fuzzywuzzy import fuzz\n",
    "    \n",
    "    matching_items = []\n",
    "    query = query.lower().split()\n",
    "    print(query)\n",
    "    for word in query:\n",
    "        for item in menuPrice.keys():\n",
    "            match_score = fuzz.token_set_ratio(word, item.lower()) # Calculate fuzzy match score between user query and coffee item\n",
    "            if match_score > 80:\n",
    "                if item not in matching_items:\n",
    "                    matching_items.append(item.title())\n",
    "    return matching_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30c2120f-f5d1-496b-b0d1-6747e0b5c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect number in user query\n",
    "def detectNumber(query):\n",
    "    num_dict = {\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5, \"six\":6, \"seven\":7, \"eight\":8, \"nine\":9, \"ten\":10} # number dictionary\n",
    "    \n",
    "    quantity = []\n",
    "    for word in query.split():\n",
    "        if word.isdigit():\n",
    "            quantity += [int(word)]\n",
    "        else:\n",
    "            for w in num_dict:\n",
    "                if w == word:\n",
    "                    quantity += [num_dict[word]]\n",
    "    return quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2e81697-6b43-4430-8778-6fc37e01cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder(query, model2, tokenizer2, lbl_encoder2):\n",
    "    prediction = model2.predict(pad_sequences(tokenizer2.texts_to_sequences([query]), truncating='post', maxlen=20), verbose=0) # remove the query from the end when its length exceed max length (20)\n",
    "    tag = lbl_encoder2.inverse_transform([np.argmax(prediction)]) #transform tag's numerical values back to string (0 -> \"greeting\")\n",
    "    print(tag)\n",
    "    if (tag == \"positive\"):\n",
    "        returnFlag = True\n",
    "        return returnFlag\n",
    "    else:\n",
    "        returnFlag = False\n",
    "        return returnFlag\n",
    "\n",
    "def transaction(queryList, model2, tokenizer2, lbl_encoder2):\n",
    "    import re\n",
    "     \n",
    "    menuPrice = {\"Espresso\": 1.50, \"Americano\": 1.50, \"Cappuccino\": 1.80, \"Latte\": 1.80, \"Macchiato\": 1.80, \"Flat White\": 2.10, \"Mocha\": 2.10, \"Black Tea\": 1.50}\n",
    "\n",
    "    lastQuery = queryList[0]\n",
    "    currentQuery = queryList[1]\n",
    "    \n",
    "    switch = True    \n",
    "    while switch == True:\n",
    "        # Detect quantity in user query\n",
    "        print(lastQuery)\n",
    "        print(currentQuery)\n",
    "        temp_query = currentQuery\n",
    "        quantity = detectNumber(temp_query)        \n",
    "        \n",
    "        print(quantity)\n",
    "        # Detect misspelled beverage\n",
    "        beverages = detectMisspelledWord(currentQuery, menuPrice)\n",
    "        print(beverages)\n",
    "        if len(beverages) == 0:\n",
    "            beverages = detectMisspelledWord(lastQuery, menuPrice)\n",
    "            print(beverages)\n",
    "        \n",
    "        if len(beverages) == 0:\n",
    "            print('\\nSorry, your order does not appear in our menu.')\n",
    "            return True\n",
    "        \n",
    "        # Assume 1 for all beverage if no number detected\n",
    "        if(len(quantity) == 0):\n",
    "            for i in beverages:\n",
    "                quantity += [1]\n",
    "                \n",
    "        # if(len(quantity) == 0):\n",
    "        #     temp_query = lastQuery\n",
    "        #     quantity = detectNumber(temp_query)    \n",
    "    \n",
    "        if len(quantity)!=0 and len(quantity) == len(beverages):\n",
    "            # Find price of each beverage\n",
    "            price = []\n",
    "            for i, j in menuPrice.items():\n",
    "                for k in beverages:\n",
    "                    if k == i:\n",
    "                        price += [j]\n",
    "\n",
    "            #Calculate total price\n",
    "            totalPrice = sum([p*q for p, q in zip(price, quantity)])\n",
    "            print(\"{:.2f}\".format(totalPrice))\n",
    "\n",
    "            # Order dictionary contains beverage, quantity, and price\n",
    "            order_dict = {(b, a): c for a, b, c in zip(beverages, quantity, price)}\n",
    "\n",
    "            # Let user confirm their order\n",
    "            # if(len(order_dict) == 0):\n",
    "            #     reorder = input('\\nSorry, your order does not appear in our menu. Would you like to reorder (yes/no)?')\n",
    "            #     prediction = model2.predict(pad_sequences(tokenizer2.texts_to_sequences([reorder]), truncating='post', maxlen=20)) # remove the query from the end when its length exceed max length (20)\n",
    "            #     tag = lbl_encoder2.inverse_transform([np.argmax(prediction)]) #transform tag's numerical values back to string (0 -> \"greeting\")\n",
    "            #     print(tag)\n",
    "            #     if (tag == \"positive\"):\n",
    "            #         return True\n",
    "            #     else:\n",
    "            #         return False\n",
    "            # else:\n",
    "            print(\"{:<10} {:<15} {:<10}\".format('Quantity', 'Beverage', 'Price'))\n",
    "            print(\"----------------------------------\\n\")\n",
    "            for i in range(len(price)):\n",
    "                print(\"{:<10} {:<15} £{:<10.2f}\".format(quantity[i], beverages[i], price[i]))\n",
    "            print('\\nTotal price is: £', \"{:.2f}\".format(totalPrice))\n",
    "            flag = True\n",
    "            while(flag == True):\n",
    "                confirmation = input('\\nGreat! I am Groovy. Please confirm your item, quantity and total price (e.g. yes/no). If you want to exit transaction, type quit. :').lower()\n",
    "                # prediction = Feedback(confirmation)\n",
    "                prediction = model2.predict(pad_sequences(tokenizer2.texts_to_sequences([confirmation]), truncating='post', maxlen=20), verbose=0) # remove the query from the end when its length exceed max length (20)\n",
    "                tag = lbl_encoder2.inverse_transform([np.argmax(prediction)]) #transform tag's numerical values back to string (0 -> \"greeting\")\n",
    "                print(tag)\n",
    "                if ('quit' in confirmation):\n",
    "                    flag = False\n",
    "                    return False\n",
    "                elif(tag == \"positive\"):\n",
    "                    flag = False\n",
    "                    print(\"Thank you! Your order has been confirmed.\")\n",
    "                    return False\n",
    "                elif(tag == \"negative\"):\n",
    "                    reorder_query = input('\\nWould you like to reorder?')\n",
    "                    returnFlag = reorder(reorder_query, model2, tokenizer2, lbl_encoder2)\n",
    "                    flag = False\n",
    "                    return returnFlag\n",
    "                else:\n",
    "                    print('\\nSorry I do not understand. Please type in \"yes\" or \"no\" to confirm your order.')\n",
    "                           \n",
    "            switch = False\n",
    "        else:\n",
    "            query = input(\"Please specify the quantity of each item (e.g. 2 mocha and 1 americano). If you want to exit transaction, type quit. :\").lower()\n",
    "            if \"quit\" in query:\n",
    "                return False\n",
    "            else:\n",
    "                lastQuery = currentQuery\n",
    "                currentQuery = query\n",
    "            \n",
    "# i=0\n",
    "# queryList = []\n",
    "# while i<2:\n",
    "#     query = input()\n",
    "#     if len(queryList) < 2:\n",
    "#         queryList.append(query.lower())\n",
    "#     i = i+1\n",
    "# flag = transaction(queryList)\n",
    "# print(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63726539-c922-4049-b2c6-18cb54d1f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedback function to detect positive and negative\n",
    "\n",
    "# Reference:\n",
    "# ****************************************************************************************************************\n",
    "#    Title: COMP3074 Human-AI Interaction Lab 3: Text classification\n",
    "#    Author: Clos, J\n",
    "#    Date: 2022\n",
    "#    Availability: https://moodle.nottingham.ac.uk/pluginfile.php/8612037/mod_resource/content/5/COMP3074_Lab3.pdf\n",
    "#\n",
    "# ****************************************************************************************************************\n",
    "\n",
    "def Feedback(query):\n",
    "    # replace short form to standard words\n",
    "    contractions = {\n",
    "        \"ain't\": \"am not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"I'll\": \"I will\",\n",
    "        \"I'll've\": \"I will have\",\n",
    "        \"I'm\": \"i am\",\n",
    "        \"I've\": \"i have\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"i'd've\": \"i would have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mightn't've\": \"might not have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you'll've\": \"you will have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"u\":\"you\",\n",
    "        \"ur\":\" your \",\n",
    "        \"n\":\" and \",\n",
    "        'bout':'about',\n",
    "        \"cn\":\"can\",\n",
    "        \"hve\":\"have\"\n",
    "    }\n",
    "\n",
    "    for word in query.split():\n",
    "        for key in contractions:\n",
    "            value = contractions[key]\n",
    "            if (word==key):\n",
    "                query = query.replace(key, value) #if query have informal words replace it with formal words\n",
    "    \n",
    "    # Read training data which contains label\n",
    "    label_dir = {\n",
    "        \"positive\": \"Dataset/positive\",\n",
    "        \"negative\": \"Dataset/negative\"\n",
    "    }\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Read all files in folders\n",
    "    for label in label_dir.keys():\n",
    "        for file in os.listdir(label_dir[label]):\n",
    "            filepath = label_dir[label] + os.sep + file\n",
    "            with open(filepath, encoding='utf8', errors='ignore', mode='r') as review:\n",
    "                content = review.read()\n",
    "                data.append(content)\n",
    "                labels.append(label)\n",
    "    \n",
    "    # Split train and test data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test, = train_test_split(data, labels, stratify=labels,\n",
    "                                                         test_size=0.25, random_state=1)\n",
    "    \n",
    "    # Split text into tokens, lower all the tokens, remove stop words, stemming, create bag-of-word models and term-document matrix:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from nltk.stem.snowball import PorterStemmer\n",
    "\n",
    "    p_stemmer = PorterStemmer()\n",
    "    analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "    def stemmed_words(doc):\n",
    "        return (p_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "    # stop word list\n",
    "    my_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "              'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are',\n",
    "              'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but',\n",
    "              'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',\n",
    "              'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
    "              'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n",
    "              'such', 'nor', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should',\n",
    "              \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'ma']\n",
    "\n",
    "    count_vect = CountVectorizer(lowercase=True, stop_words = my_list)\n",
    "    X_train_counts = count_vect.fit_transform(X_train)\n",
    "    \n",
    "    # Weighting\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer(use_idf=True, sublinear_tf=True).fit(X_train_counts)\n",
    "    X_train_tf = tfidf_transformer.transform(X_train_counts)\n",
    "    \n",
    "    # Traning a classifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    classifier = LogisticRegression(random_state=0).fit(X_train_tf, y_train) \n",
    "        \n",
    "    #Evaluation of classifier\n",
    "    from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "    # Preprocessing documents and creating term-document matrix\n",
    "    X_new_counts = count_vect.transform(X_test)\n",
    "\n",
    "    # Weighting\n",
    "    X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "    # Predict on the test set\n",
    "    predicted = classifier.predict(X_new_tfidf)\n",
    "\n",
    "    # Print confusion matrix, accuracy, and f1 score\n",
    "    # print(confusion_matrix(y_test, predicted))\n",
    "    # print(accuracy_score(y_test, predicted))\n",
    "    # print(f1_score(y_test, predicted, pos_label='positive'))\n",
    "    \n",
    "    #Process query\n",
    "    query = [query]\n",
    "    processed_query = count_vect.transform(query)\n",
    "\n",
    "    # Weighting for query\n",
    "    processed_query = tfidf_transformer.transform(processed_query)\n",
    "    \n",
    "    # Predict query's emotion\n",
    "    prediction = classifier.predict(processed_query)\n",
    "    # print(prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4df368fb-20f1-4e78-a8ad-63b60f6a79cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hello, welcome to Chin's Coffee House! My name is Groovy. What is your name? :  Hong Shen\n",
      "\n",
      "Hi Hong Shen, I am Groovy. Welcome to Chin's Coffee House. How can I help you? If you want to exit, type quit. : quit\n",
      "\n",
      "Groovy: Thank you for visiting Chin's Coffee House. What do you think of my service? : good\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thank you for your feedback, I am glad to hear that!\n",
      "Bye, take care Hong Shen\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def chat():\n",
    "    # load trained model\n",
    "    # model = keras.models.load_model('chat_model')\n",
    "\n",
    "    # load tokenizer object\n",
    "#     with open('tokenizer.pickle', 'rb') as handle:\n",
    "#         tokenizer = pickle.load(handle)\n",
    "\n",
    "#     # load label encoder object\n",
    "#     with open('label_encoder.pickle', 'rb') as enc:\n",
    "#         lbl_encoder = pickle.load(enc)\n",
    "    \n",
    "    with open('intentTest.json', encoding='utf-8') as file:\n",
    "        data1 = json.load(file)\n",
    "        \n",
    "    with open('TrueFalseData.json', encoding='utf-8') as file1:\n",
    "        data2 = json.load(file1)\n",
    "    \n",
    "    model1, lbl_encoder1, tokenizer1 = model(data1)\n",
    "    model2, lbl_encoder2, tokenizer2 = model(data2)\n",
    "    \n",
    "    # parameters\n",
    "    max_len = 20\n",
    "    queryList = [\"\",\"\"]\n",
    "    newName = \"\"\n",
    "    flag1 = True\n",
    "    flag2 = False\n",
    "    query = input(\"Hello, welcome to Chin's Coffee House! My name is Groovy. What is your name? : \")\n",
    "    name = NER(query)\n",
    "    while flag1:\n",
    "        if flag2 == True:\n",
    "            query = input('\\nLuna: What would you like to order? If you want to exit, type quit. :')\n",
    "            flag2 = False\n",
    "        else:\n",
    "            query = input('\\nHi'+ (newName if len(newName)!=0 else name) +\", I am Groovy. Welcome to Chin's Coffee House. How can I help you? If you want to exit, type quit. :\")\n",
    "            \n",
    "        if query.lower() == \"quit\":\n",
    "            flag1 = False\n",
    "            query = input(\"\\nGroovy: Thank you for visiting Chin's Coffee House. What do you think of my service? :\")\n",
    "            prediction = Feedback(query)\n",
    "            if(prediction=='positive'):\n",
    "                print('\\nThank you for your feedback, I am glad to hear that!')\n",
    "            else:\n",
    "                print('\\nI am sorry to hear that, I will feedback to my company.')\n",
    "            print(\"Bye, take care\"+ (newName if len(newName)!=0 else name))\n",
    "        else:\n",
    "            queryList[0] = queryList[1]\n",
    "            queryList[1] = query.lower()\n",
    "            \n",
    "            # model, lbl_encoder, tokenizer = preprocessing(data)\n",
    "            result = model1.predict(pad_sequences(tokenizer1.texts_to_sequences([query]), truncating='post', maxlen=max_len), verbose=0) # remove the query from the end when its length exceed max length (20)\n",
    "            tag = lbl_encoder1.inverse_transform([np.argmax(result)]) #transform tag's numerical values back to string (0 -> \"greeting\")\n",
    "\n",
    "            for i in data1['intents']:\n",
    "                if i['tag'] == tag:\n",
    "                    print(tag)\n",
    "                    response = np.random.choice(i['responses'])\n",
    "                    if response == \"Menu\":\n",
    "                        openMenu()                  \n",
    "                        flag2 = True\n",
    "                    elif response == \"transaction\":\n",
    "                        print(queryList)\n",
    "                        flag2 = transaction(queryList, model2, tokenizer2, lbl_encoder2)\n",
    "                    elif response == \"username\":\n",
    "                        print(\"Your name is\" + (newName if len(newName)!= 0 else name) + \".\")\n",
    "                    elif response == \"Change name\":\n",
    "                        newName = NER(query)\n",
    "                        print(\"Noted, your name is\" + (newName if len(newName)!= 0 else name) + \".\")\n",
    "                    else:\n",
    "                        print(\"Groovy:\", response)\n",
    "    \n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e96f6-bea6-40f1-8e79-b32d61cc8a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5491f-4ca4-4154-8095-6b2c463a4e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
